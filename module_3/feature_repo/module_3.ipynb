{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Scheduling batch transformations with dbt, Airflow, and Feast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview\n",
    "In this notebook, we see how to use dbt to automatically run batch transformations with Airflow, and run materialization once dbt has run its incremental model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../architecture.png\" width=\"750\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup the feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%env SNOWFLAKE_DEPLOYMENT_URL=\"[YOUR DEPLOYMENT]\"\n",
    "%env SNOWFLAKE_USER=\"[YOUR USER]\"\n",
    "%env SNOWFLAKE_PASSWORD=\"[YOUR PASSWORD]\"\n",
    "%env SNOWFLAKE_ROLE=\"[YOUR ROLE]\"\n",
    "%env SNOWFLAKE_WAREHOUSE=\"[YOUR WAREHOUSE]\"\n",
    "%env SNOWFLAKE_DATABASE=\"[YOUR DATABASE]\"\n",
    "%env USAGE=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast import FeatureStore\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "store = FeatureStore(repo_path=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch training data from offline store\n",
    "Just to verify the features are in the batch sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        USER_ID            event_timestamp CREDIT_SCORE  7D_AVG_AMT\n",
      "0   C1316679623 2021-07-14 02:21:31.267597          692    2454.810\n",
      "1   C1817064970 2021-07-14 10:47:53.598600          692  232973.540\n",
      "2   C1137064844 2021-07-14 02:24:54.111738          662  203089.480\n",
      "3    C277832456 2021-07-15 23:36:52.611674          624  322265.510\n",
      "4   C1629780233 2021-07-15 07:50:16.902199          578   81683.840\n",
      "5   C1880755277 2021-07-15 00:36:09.758792          626   73723.490\n",
      "6    C596660112 2021-07-14 10:48:42.592094          652   55033.350\n",
      "7   C1622805037 2021-07-15 16:04:14.752046          653  261867.150\n",
      "8   C1721218418 2021-07-15 07:47:29.340135          643  212377.195\n",
      "9    C573703815 2021-07-15 10:52:38.382194          647  166923.465\n",
      "10  C1700064043 2021-07-15 06:10:45.897233          672  184942.440\n",
      "11  C1693799911 2021-07-15 06:11:17.415039          638   25347.370\n",
      "12  C1258929790 2021-07-15 10:50:22.805955          641  175895.690\n",
      "13   C917254266 2021-07-14 10:13:42.264576          605  102720.530\n",
      "14   C700541060 2021-07-14 10:14:10.430790          686    1750.720\n",
      "15   C693393195 2021-07-14 05:59:42.636102          583  483107.810\n",
      "16   C888690253 2021-07-15 13:32:09.235399          670  229381.800\n",
      "17  C1217230511 2021-07-14 06:00:35.761601          638  112035.970\n",
      "18  C1332319768 2021-07-14 06:00:39.610000          662  808677.560\n",
      "19   C665145356 2021-07-14 03:37:32.708407          674   90043.560\n"
     ]
    }
   ],
   "source": [
    "entity_sql = f\"\"\"\n",
    "    SELECT\n",
    "        NAMEORIG as USER_ID,\n",
    "        TIMESTAMP as \"event_timestamp\"\n",
    "    FROM {store.get_data_source(\"transactions_source\").get_table_query_string()}\n",
    "    WHERE TIMESTAMP BETWEEN '2021-07-14 00:00:00+0' and '2021-07-16 00:00:00+0'\n",
    "\"\"\"\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_sql,\n",
    "    features=store.get_feature_service(\"model_v2\"),\n",
    ").to_df()\n",
    "print(training_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Materialize batch features & fetch online features from Redis\n",
    "We didn't materialize the full set of data with Airflow to save time / money. Now we selectively materialize so we can fetch the right online data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dannychiao/.pyenv/versions/3.10.3/envs/python-3.10/lib/python3.10/site-packages/requests_toolbelt/_compat.py:56: DeprecationWarning: 'urllib3.contrib.pyopenssl' module is deprecated and will be removed in a future release of urllib3 2.x. Read more in this issue: https://github.com/urllib3/urllib3/issues/2680\n",
      "  from requests.packages.urllib3.contrib.pyopenssl \\\n",
      "Materializing \u001b[1m\u001b[32m2\u001b[0m feature views from \u001b[1m\u001b[32m2021-07-13 17:00:00-07:00\u001b[0m to \u001b[1m\u001b[32m2021-07-15 17:00:00-07:00\u001b[0m into the \u001b[1m\u001b[32mredis\u001b[0m online store.\n",
      "\n",
      "\u001b[1m\u001b[32maggregate_transactions_features\u001b[0m:\n",
      "100%|██████████████████████████████████████████████████████| 54991/54991 [00:01<00:00, 28759.14it/s]\n",
      "\u001b[1m\u001b[32mcredit_scores_features\u001b[0m:\n",
      "100%|████████████████████████████████████████████████████| 654482/654482 [00:22<00:00, 29090.35it/s]\n"
     ]
    }
   ],
   "source": [
    "!feast materialize \"2021-07-14 00:00:00+0\" \"2021-07-16 00:00:00+0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SDK based online retrieval\n",
    "Now we can retrieve these materialized features from Redis by directly using the SDK. This is one of the most popular ways to retrieve features with Feast since it allows you to integrate with an existing service (e.g. a Flask) that also handles model inference or pre/post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7D_AVG_AMT  :  [58164.1796875]\n",
      "CREDIT_SCORE  :  [659]\n",
      "USER_ID  :  ['C1783349759']\n"
     ]
    }
   ],
   "source": [
    "features = store.get_online_features(\n",
    "    features=store.get_feature_service(\"model_v2\"),\n",
    "    entity_rows=[\n",
    "        {\n",
    "            \"USER_ID\": \"C1783349759\",\n",
    "        }\n",
    "    ],\n",
    ").to_dict()\n",
    "\n",
    "def print_online_features(features):\n",
    "    for key, value in sorted(features.items()):\n",
    "        print(key, \" : \", value)\n",
    "\n",
    "print_online_features(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HTTP based online retrieval\n",
    "We can also retrieve from a deployed feature server. We had previously deployed this with Docker Compose (see [docker-compose.yml](../docker-compose.yml))\n",
    "\n",
    "This can be preferable for many reasons. If you want to build an in-memory cache, caching on a central feature server can allow more effective caching across teams. You can also more centrally manage rate-limiting / access control, upgrade Feast versions independently, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"metadata\": {\n",
      "        \"feature_names\": [\n",
      "            \"USER_ID\",\n",
      "            \"CREDIT_SCORE\",\n",
      "            \"7D_AVG_AMT\"\n",
      "        ]\n",
      "    },\n",
      "    \"results\": [\n",
      "        {\n",
      "            \"event_timestamps\": [\n",
      "                \"1970-01-01T00:00:00Z\"\n",
      "            ],\n",
      "            \"statuses\": [\n",
      "                \"PRESENT\"\n",
      "            ],\n",
      "            \"values\": [\n",
      "                \"C1783349759\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"event_timestamps\": [\n",
      "                \"2021-07-14T00:55:21Z\"\n",
      "            ],\n",
      "            \"statuses\": [\n",
      "                \"PRESENT\"\n",
      "            ],\n",
      "            \"values\": [\n",
      "                659\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"event_timestamps\": [\n",
      "                \"2021-07-14T06:00:58Z\"\n",
      "            ],\n",
      "            \"statuses\": [\n",
      "                \"PRESENT\"\n",
      "            ],\n",
      "            \"values\": [\n",
      "                58164.1796875\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "online_request = {\n",
    "  \"feature_service\": \"model_v2\",\n",
    "  \"entities\": {\n",
    "    \"USER_ID\": [\"C1783349759\"]\n",
    "  }\n",
    "}\n",
    "r = requests.post('http://localhost:6566/get-online-features', data=json.dumps(online_request))\n",
    "print(json.dumps(r.json(), indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generating fresher features via stream transformations\n",
    "Now we push streaming features into Feast by ingesting events from Kafka and processing with Spark Structured Streaming.\n",
    "- These features can then be further post-processed and combined with other features or request data in on demand transforms.\n",
    "- An example might be to push in the last 5 transactions, and in on demand transforms generate the average of those transactions.\n",
    "\n",
    "Feast will help manage both batch and streaming sources for you. You can run `feast materialize-incremental` as well as ingest streaming features to the same online store.\n",
    "\n",
    "### 5a. Connect to Kafka from Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/dannychiao/.pyenv/versions/3.10.3/envs/python-3.10/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/dannychiao/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/dannychiao/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e0d857e0-2b24-428c-a443-15ba8d6c2920;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in local-m2-cache\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in local-m2-cache\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 333ms :: artifacts dl 12ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from local-m2-cache in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from local-m2-cache in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e0d857e0-2b24-428c-a443-15ba8d6c2920\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/6ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/24 23:34:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages=org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell\"\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"feast-spark\").getOrCreate()\n",
    "# Reduce partitions since default is 200 which will be slow on a local machine\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "\n",
    "schema = (\n",
    "    \"STRUCT<\"\n",
    "        \"amount: DOUBLE, \"\n",
    "        \"isFlaggedFraud: BIGINT, \"\n",
    "        \"isFraud: BIGINT, \"\n",
    "        \"nameDest: STRING, \"\n",
    "        \"nameOrig: STRING, \"\n",
    "        \"timestamp: TIMESTAMP, \"\n",
    "        \"type_CASH_IN: BIGINT, \"\n",
    "        \"type_CASH_OUT: BIGINT, \"\n",
    "        \"type_DEBIT: BIGINT, \"\n",
    "        \"type_PAYMENT: BIGINT, \"\n",
    "        \"type_TRANSFER: BIGINT\"\n",
    "    \">\"\n",
    ")\n",
    "\n",
    "# Subscribe to 1 topic, with headers\n",
    "df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"subscribe\", \"transactions\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    "    .selectExpr('CAST(value AS STRING)')\n",
    "    .select(from_json('value', schema).alias(\"temp\"))\n",
    "    .select(\"temp.*\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/24 23:34:07 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "22/10/24 23:34:12 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/24 23:34:17 WARN HDFSBackedStateStoreProvider: The state for version 5 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "22/10/24 23:34:17 WARN HDFSBackedStateStoreProvider: The state for version 5 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=======================>                                   (2 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/24 23:34:17 WARN HDFSBackedStateStoreProvider: The state for version 5 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:===================================>                       (3 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/24 23:34:18 WARN HDFSBackedStateStoreProvider: The state for version 5 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "22/10/24 23:34:18 WARN HDFSBackedStateStoreProvider: The state for version 5 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing window\n",
      "                      TIMESTAMP     7D_AVG_AMT\n",
      "USER_ID                                       \n",
      "C1002549077 2024-09-08 06:00:00  492373.865000\n",
      "C1017359561 2024-09-08 06:00:00  167760.910000\n",
      "C1029224019 2024-09-08 06:00:00  170918.001000\n",
      "C103805766  2024-09-08 06:00:00  167167.800000\n",
      "C1054169081 2024-09-08 06:00:00  259062.940000\n",
      "...                         ...            ...\n",
      "C975618271  2024-09-08 06:00:00   93361.601429\n",
      "C976047598  2024-09-08 06:00:00   12879.980000\n",
      "C980973980  2024-09-08 06:00:00    2703.550000\n",
      "C988211716  2024-09-08 06:00:00  114029.610000\n",
      "C989890503  2024-09-08 06:00:00  186932.563333\n",
      "\n",
      "[259 rows x 2 columns]\n",
      "Num rows: 259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing window\n",
      "                      TIMESTAMP     7D_AVG_AMT\n",
      "USER_ID                                       \n",
      "C1002549077 2024-09-08 07:00:00  451469.744286\n",
      "C1017359561 2024-09-08 07:00:00  167760.910000\n",
      "C1029224019 2024-09-08 07:00:00  170918.001000\n",
      "C103805766  2024-09-08 07:00:00  167167.800000\n",
      "C1054169081 2024-09-08 07:00:00  259062.940000\n",
      "...                         ...            ...\n",
      "C975618271  2024-09-08 07:00:00   93361.601429\n",
      "C976047598  2024-09-08 07:00:00   12879.980000\n",
      "C980973980  2024-09-08 07:00:00    2703.550000\n",
      "C988211716  2024-09-08 07:00:00  114029.610000\n",
      "C989890503  2024-09-08 07:00:00  186932.563333\n",
      "\n",
      "[266 rows x 2 columns]\n",
      "Num rows: 266\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def send_to_feast(df, epoch):\n",
    "    pandas_df: pd.DataFrame = df.toPandas()\n",
    "    if pandas_df.empty:\n",
    "        return\n",
    "    \n",
    "    if \"TIMESTAMP\" in pandas_df:\n",
    "        print(\"processing window\")\n",
    "        # Filter out only for the latest window for the user_id\n",
    "        pandas_df = pandas_df.sort_values(by=[\"USER_ID\",\"TIMESTAMP\"], ascending=False).groupby(\"USER_ID\").nth(-1)\n",
    "        store.push(\"transactions_7d\", pandas_df)\n",
    "        print(pandas_df)\n",
    "    print(f\"Num rows: {len(pandas_df.index)}\")\n",
    "\n",
    "seven_day_avg = (\n",
    "    df.withWatermark(\"timestamp\", \"1 second\") \n",
    "        .groupBy(\"nameOrig\", window(timeColumn=\"timestamp\", windowDuration=\"7 day\", slideDuration=\"1 hour\"))\n",
    "        .agg(\n",
    "            avg(\"amount\").alias(\"7D_AVG_AMT\")\n",
    "        )\n",
    "        .select(col(\"nameOrig\").alias(\"USER_ID\"), col(\"window.end\").alias(\"TIMESTAMP\"), \"7D_AVG_AMT\")\n",
    ")\n",
    "\n",
    "query_1 = (\n",
    "    seven_day_avg\n",
    "        .writeStream\n",
    "        .outputMode(\"append\") \n",
    "        .option(\"checkpointLocation\", \"/tmp/feast-workshop/q3/\")\n",
    "        .trigger(processingTime=\"30 seconds\")\n",
    "        .foreachBatch(send_to_feast)\n",
    "        .start()\n",
    ")\n",
    "\n",
    "query_1.awaitTermination(timeout=30)\n",
    "query_1.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURES_OLD ===\n",
      "7D_AVG_AMT  :  [58164.1796875]\n",
      "CREDIT_SCORE  :  [659]\n",
      "USER_ID  :  ['C1783349759']\n",
      "=== FEATURES_NEW ===\n",
      "7D_AVG_AMT  :  [105369.8125]\n",
      "CREDIT_SCORE  :  [659]\n",
      "USER_ID  :  ['C1783349759']\n"
     ]
    }
   ],
   "source": [
    "features_new = store.get_online_features(\n",
    "    features=store.get_feature_service(\"model_v2\"),\n",
    "    entity_rows=[\n",
    "        {\n",
    "            \"USER_ID\": \"C1783349759\",\n",
    "        }\n",
    "    ],\n",
    ").to_dict()\n",
    "\n",
    "print(\"=== FEATURES_OLD ===\")\n",
    "print_online_features(features)\n",
    "print(\"=== FEATURES_NEW ===\")\n",
    "print_online_features(features_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "dir_path = '/tmp/feast-workshop/'\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(dir_path)\n",
    "except OSError as e:\n",
    "    print(\"Error: %s : %s\" % (dir_path, e.strerror))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit ('python-3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85bfe6b94e566a7f5b4e7cc6a85d6f54a419de6a6cf36e6a81de40ec843b83e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
