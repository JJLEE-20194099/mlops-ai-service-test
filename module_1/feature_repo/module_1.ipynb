{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Getting started with Feast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Intro to Feast\n",
    "- What is a feature view?\n",
    "- What is the Feast registry?\n",
    "    - Features as code (e.g. registering features that can be re-used elsewhere)\n",
    "- Major components of Feast (offline store, online store, materialization, data sources, feature server)\n",
    "- Batch data sources vs streaming sources\n",
    "\n",
    "[Feast Quickstart](https://colab.research.google.com/github/feast-dev/feast/blob/master/examples/quickstart/quickstart.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ingesting stream Setup Spark Structured Streaming to read this Kafka Topic\n",
    "We first read in the events, apply the schema, run some transformations, and forEachBatch push to feast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages=org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/dannychiao/.pyenv/versions/3.7.10/envs/python-3.7/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/dannychiao/.pyenv/versions/3.7.10/envs/python-3.7/lib/python3.7/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/dannychiao/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/dannychiao/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0c7bfbbf-c37d-4b79-ae54-87167f0f2485;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in local-m2-cache\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in local-m2-cache\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 448ms :: artifacts dl 13ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from local-m2-cache in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from local-m2-cache in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0c7bfbbf-c37d-4b79-ae54-87167f0f2485\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/9ms)\n",
      "22/05/12 22:15:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/12 22:15:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"feast-spark\").getOrCreate()\n",
    "# Reduce partitions since default is 200 which will be slow on a local machine\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "\n",
    "schema = (\n",
    "    StructType()\n",
    "        .add('driver_id', IntegerType(), False)\n",
    "        .add('miles_driven', DoubleType(), False)\n",
    "        .add('event_timestamp', TimestampType(), False)\n",
    "        .add('conv_rate', DoubleType(), False)\n",
    "        .add('acc_rate', DoubleType(), False)\n",
    ")\n",
    "\n",
    "# Subscribe to 1 topic, with headers\n",
    "df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"subscribe\", \"drivers\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    "    .selectExpr('CAST(value AS STRING)')\n",
    "    .select(from_json('value', schema).alias(\"temp\"))\n",
    "    .select(\"temp.*\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Feast\n",
    "We now instantiate a Feast `FeatureStore` object to push data to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_online_features(features):\n",
    "    for key, value in sorted(features.items()):\n",
    "        print(key, \" : \", value)\n",
    "\n",
    "from feast import FeatureStore\n",
    "from datetime import datetime\n",
    "\n",
    "store = FeatureStore(repo_path=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch training data from offline store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      driver_id                  event_timestamp  conv_rate  acc_rate  \\\n",
      "360        1001        2021-04-12 10:59:42+00:00   0.521149  0.751659   \n",
      "721        1002        2021-04-12 08:12:10+00:00   0.089014  0.212637   \n",
      "1084       1003        2021-04-12 16:40:26+00:00   0.188855  0.344736   \n",
      "1445       1004        2021-04-12 15:01:12+00:00   0.296492  0.935305   \n",
      "1805       1001 2022-05-12 22:15:47.590475+00:00   0.404588  0.407571   \n",
      "\n",
      "      daily_miles_driven  \n",
      "360            18.926695  \n",
      "721            12.005569  \n",
      "1084           23.490234  \n",
      "1445           19.204191  \n",
      "1805          350.650257  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "entity_df = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"driver_id\": [1001, 1002, 1003, 1004, 1001],\n",
    "        \"event_timestamp\": [\n",
    "            datetime(2021, 4, 12, 10, 59, 42),\n",
    "            datetime(2021, 4, 12, 8, 12, 10),\n",
    "            datetime(2021, 4, 12, 16, 40, 26),\n",
    "            datetime(2021, 4, 12, 15, 1, 12),\n",
    "            datetime.now()\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"driver_hourly_stats:conv_rate\",\n",
    "        \"driver_hourly_stats:acc_rate\",\n",
    "        \"driver_daily_features:daily_miles_driven\"\n",
    "    ],\n",
    ").to_df()\n",
    "print(training_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch online features from Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_rate  :  [0.4075707495212555]\n",
      "conv_rate  :  [0.4045884609222412]\n",
      "daily_miles_driven  :  [350.6502685546875]\n",
      "driver_id  :  [1001]\n"
     ]
    }
   ],
   "source": [
    "features = store.get_online_features(\n",
    "    features=[\n",
    "        \"driver_hourly_stats:conv_rate\",\n",
    "        \"driver_hourly_stats:acc_rate\",\n",
    "        \"driver_daily_features:daily_miles_driven\",\n",
    "    ],\n",
    "    entity_rows=[\n",
    "        {\n",
    "            \"driver_id\": 1001,\n",
    "        }\n",
    "    ],\n",
    ").to_dict()\n",
    "print_online_features(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing transformed features to Feast\n",
    "Now we push events into Feast, which can then be further post-processed in on demand transforms.\n",
    "\n",
    "An example might be to push in the last 5 transactions, and in on demand transforms generate the average of those transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/12/2022 10:16:06 PM INFO:Callback Server Starting\n",
      "05/12/2022 10:16:06 PM INFO:Socket listening on ('127.0.0.1', 63229)\n",
      "22/05/12 22:16:06 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "05/12/2022 10:16:13 PM INFO:Python Server ready to receive messages\n",
      "05/12/2022 10:16:13 PM INFO:Received command c on object id p0\n",
      "/Users/dannychiao/.pyenv/versions/3.7.10/envs/python-3.7/lib/python3.7/site-packages/pyspark/sql/pandas/utils.py:35: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing window\n",
      "              event_timestamp  daily_miles_driven                    created\n",
      "driver_id                                                                   \n",
      "1001      2022-04-11 08:00:00           37.853390 2022-05-13 02:16:16.518250\n",
      "1002      2022-04-11 08:00:00           24.011137 2022-05-13 02:16:16.518250\n",
      "1003      2022-04-11 08:00:00           46.980469 2022-05-13 02:16:16.518250\n",
      "1004      2022-04-11 08:00:00           38.408382 2022-05-13 02:16:16.518250\n",
      "1005      2022-04-11 08:00:00           11.529007 2022-05-13 02:16:16.518250\n",
      "Num rows: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dannychiao/GitHub/feast/sdk/python/feast/feature_store.py:1223: RuntimeWarning: Push source is an experimental feature. This API is unstable and it could and might change in the future. We do not guarantee that future changes will maintain backward compatibility.\n",
      "  RuntimeWarning,\n",
      "05/12/2022 10:16:17 PM INFO:Received command c on object id p0\n",
      "/Users/dannychiao/.pyenv/versions/3.7.10/envs/python-3.7/lib/python3.7/site-packages/pyspark/sql/pandas/utils.py:35: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "05/12/2022 10:16:30 PM INFO:Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "def send_to_feast(df):\n",
    "    pandas_df: pd.DataFrame = df.toPandas()\n",
    "    if pandas_df.empty:\n",
    "        return\n",
    "    \n",
    "    if \"end\" in pandas_df:\n",
    "        print(\"processing window\")\n",
    "        # Filter out only for the latest window for the driver id\n",
    "        pandas_df = pandas_df.sort_values(by=[\"driver_id\",\"end\"], ascending=False).groupby(\"driver_id\").nth(-1)\n",
    "        pandas_df = pandas_df.rename(columns = {\"end\": \"event_timestamp\"})\n",
    "        pandas_df['created'] = pd.to_datetime('now')\n",
    "        store.push(\"driver_stats_push_source\", pandas_df)\n",
    "    pandas_df.sort_values(by=\"driver_id\", inplace=True)\n",
    "    print(pandas_df.head(20))\n",
    "    print(f\"Num rows: {len(pandas_df.index)}\")\n",
    "\n",
    "avg_conv_rate = (\n",
    "    df.withWatermark(\"event_timestamp\", \"1 second\")\n",
    "        .groupBy(\"driver_id\")\n",
    "        .agg(max(\"event_timestamp\").alias(\"timestamp\"), count(\"event_timestamp\").alias(\"num_rows\"), avg(\"conv_rate\"), sum(\"acc_rate\"))\n",
    ")\n",
    "\n",
    "daily_miles_driven = (\n",
    "    df.withWatermark(\"event_timestamp\", \"1 second\") \n",
    "        .groupBy(\"driver_id\", window(timeColumn=\"event_timestamp\", windowDuration=\"1 day\", slideDuration=\"1 hour\"))\n",
    "        .agg(sum(\"miles_driven\").alias(\"daily_miles_driven\"))\n",
    "        .select(\"driver_id\", \"window.end\", \"daily_miles_driven\")\n",
    ")\n",
    "\n",
    "query_1 = daily_miles_driven \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/feast-workshop/q1/\") \\\n",
    "    .trigger(processingTime=\"15 seconds\") \\\n",
    "    .foreachBatch(send_to_feast) \\\n",
    "    .start()\n",
    "\n",
    "query_1.awaitTermination(timeout=30)\n",
    "query_1.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_rate  :  [0.4075707495212555]\n",
      "conv_rate  :  [0.4045884609222412]\n",
      "daily_miles_driven  :  [37.853389739990234]\n",
      "driver_id  :  [1001]\n"
     ]
    }
   ],
   "source": [
    "features = store.get_online_features(\n",
    "    features=[\n",
    "        \"driver_hourly_stats:conv_rate\",\n",
    "        \"driver_hourly_stats:acc_rate\",\n",
    "        \"driver_daily_features:daily_miles_driven\",\n",
    "    ],\n",
    "    entity_rows=[\n",
    "        {\n",
    "            \"driver_id\": 1001,\n",
    "        }\n",
    "    ],\n",
    ").to_dict()\n",
    "print_online_features(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "dir_path = '/tmp/feast-workshop/q1/'\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(dir_path)\n",
    "except OSError as e:\n",
    "    print(\"Error: %s : %s\" % (dir_path, e.strerror))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa281768287d6a9f4d6a9f38bdad903cf8826d866a1bab281f248e07fa174c1b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('python-3.7')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
