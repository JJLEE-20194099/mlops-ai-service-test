{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Serving fresh online features with Feast, Kafka, Redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview\n",
    "In this notebook, we explore using Spark to build streaming features from events in Kafka and registering them within Feast. We then showcase how Feast combines these streaming features with batch data sources in the online store (Redis). Users can then retrieve features at low latency from Redis through Feast.\n",
    "\n",
    "If you haven't already, look at the [README](../README.md) for setup instructions prior to starting this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Spark Structured Streaming to read this Kafka Topic\n",
    "We first read in the events, apply the schema, run some transformations, and `forEachBatch` push to Feast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages=org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"feast-spark\").getOrCreate()\n",
    "# Reduce partitions since default is 200 which will be slow on a local machine\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "\n",
    "schema = (\n",
    "    StructType()\n",
    "        .add('driver_id', IntegerType(), False)\n",
    "        .add('miles_driven', DoubleType(), False)\n",
    "        .add('event_timestamp', TimestampType(), False)\n",
    "        .add('conv_rate', DoubleType(), False)\n",
    "        .add('acc_rate', DoubleType(), False)\n",
    ")\n",
    "\n",
    "# Subscribe to 1 topic, with headers\n",
    "df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"subscribe\", \"drivers\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    "    .selectExpr('CAST(value AS STRING)')\n",
    "    .select(from_json('value', schema).alias(\"temp\"))\n",
    "    .select(\"temp.*\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Setup the feature store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply feature repository\n",
    "We first run `feast apply` to register the data sources + features and setup Redis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created entity \u001b[1m\u001b[32mdriver\u001b[0m\n",
      "Created feature view \u001b[1m\u001b[32mdriver_hourly_stats\u001b[0m\n",
      "Created feature view \u001b[1m\u001b[32mdriver_daily_features\u001b[0m\n",
      "Created on demand feature view \u001b[1m\u001b[32mtransformed_conv_rate\u001b[0m\n",
      "Created feature service \u001b[1m\u001b[32mmodel_v2\u001b[0m\n",
      "\n",
      "Deploying infrastructure for \u001b[1m\u001b[32mdriver_hourly_stats\u001b[0m\n",
      "Deploying infrastructure for \u001b[1m\u001b[32mdriver_daily_features\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!feast apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we instantiate a Feast `FeatureStore` object to push data to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast import FeatureStore\n",
    "from datetime import datetime\n",
    "\n",
    "store = FeatureStore(repo_path=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch training data from offline store\n",
    "Just to verify the features are in the batch sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      driver_id                  event_timestamp  conv_rate  acc_rate  \\\n",
      "360        1001        2021-04-12 10:59:42+00:00   0.521149  0.751659   \n",
      "721        1002        2021-04-12 08:12:10+00:00   0.089014  0.212637   \n",
      "1084       1003        2021-04-12 16:40:26+00:00   0.188855  0.344736   \n",
      "1445       1004        2021-04-12 15:01:12+00:00   0.296492  0.935305   \n",
      "1805       1001 2022-05-14 01:52:59.452719+00:00   0.404588  0.407571   \n",
      "\n",
      "      daily_miles_driven  \n",
      "360            18.926695  \n",
      "721            12.005569  \n",
      "1084           23.490234  \n",
      "1445           19.204191  \n",
      "1805          350.650257  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "entity_df = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"driver_id\": [1001, 1002, 1003, 1004, 1001],\n",
    "        \"event_timestamp\": [\n",
    "            datetime(2021, 4, 12, 10, 59, 42),\n",
    "            datetime(2021, 4, 12, 8, 12, 10),\n",
    "            datetime(2021, 4, 12, 16, 40, 26),\n",
    "            datetime(2021, 4, 12, 15, 1, 12),\n",
    "            datetime.now()\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"driver_hourly_stats:conv_rate\",\n",
    "        \"driver_hourly_stats:acc_rate\",\n",
    "        \"driver_daily_features:daily_miles_driven\"\n",
    "    ],\n",
    ").to_df()\n",
    "print(training_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Materialize batch features & fetch online features from Redis\n",
    "First we materialize features (which generate the latest values for each entity key from batch sources) into the online store (Redis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materializing \u001b[1m\u001b[32m2\u001b[0m feature views to \u001b[1m\u001b[32m2022-05-13 20:00:00-04:00\u001b[0m into the \u001b[1m\u001b[32mredis\u001b[0m online store.\n",
      "\n",
      "\u001b[1m\u001b[32mdriver_hourly_stats\u001b[0m from \u001b[1m\u001b[32m1748-07-29 05:53:04-04:56:02\u001b[0m to \u001b[1m\u001b[32m2022-05-13 20:00:00-04:00\u001b[0m:\n",
      "100%|████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 950.74it/s]\n",
      "\u001b[1m\u001b[32mdriver_daily_features\u001b[0m from \u001b[1m\u001b[32m1748-07-29 05:53:04-04:56:02\u001b[0m to \u001b[1m\u001b[32m2022-05-13 20:00:00-04:00\u001b[0m:\n",
      "100%|███████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 1127.20it/s]\n"
     ]
    }
   ],
   "source": [
    "!feast materialize-incremental $(date +%Y-%m-%d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can retrieve these features from Redis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_rate  :  [0.4075707495212555]\n",
      "conv_rate  :  [0.4045884609222412]\n",
      "daily_miles_driven  :  [350.6502685546875]\n",
      "driver_id  :  [1001]\n"
     ]
    }
   ],
   "source": [
    "features = store.get_online_features(\n",
    "    features=[\n",
    "        \"driver_hourly_stats:conv_rate\",\n",
    "        \"driver_hourly_stats:acc_rate\",\n",
    "        \"driver_daily_features:daily_miles_driven\",\n",
    "    ],\n",
    "    entity_rows=[\n",
    "        {\n",
    "            \"driver_id\": 1001,\n",
    "        }\n",
    "    ],\n",
    ").to_dict()\n",
    "\n",
    "def print_online_features(features):\n",
    "    for key, value in sorted(features.items()):\n",
    "        print(key, \" : \", value)\n",
    "\n",
    "print_online_features(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generating fresher features via stream transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. Building streaming features with Kafka + Spark Structured Streaming\n",
    "Now we push streaming features into Feast by ingesting events from Kafka and processing with Spark Structured Streaming.\n",
    "- These features can then be further post-processed and combined with other features or request data in on demand transforms.\n",
    "- An example might be to push in the last 5 transactions, and in on demand transforms generate the average of those transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/14 01:53:51 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "/Users/dannychiao/.pyenv/versions/3.7.10/envs/python-3.7/lib/python3.7/site-packages/pyspark/sql/pandas/utils.py:35: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing window\n",
      "              event_timestamp  daily_miles_driven                    created\n",
      "driver_id                                                                   \n",
      "1001      2022-04-11 08:00:00           18.926695 2022-05-14 05:53:54.228986\n",
      "1002      2022-04-11 08:00:00           12.005569 2022-05-14 05:53:54.228986\n",
      "1003      2022-04-11 08:00:00           23.490234 2022-05-14 05:53:54.228986\n",
      "1004      2022-04-11 08:00:00           19.204191 2022-05-14 05:53:54.228986\n",
      "1005      2022-04-11 08:00:00            5.764504 2022-05-14 05:53:54.228986\n",
      "Num rows: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dannychiao/.pyenv/versions/3.7.10/envs/python-3.7/lib/python3.7/site-packages/feast/feature_store.py:1223: RuntimeWarning: Push source is an experimental feature. This API is unstable and it could and might change in the future. We do not guarantee that future changes will maintain backward compatibility.\n",
      "  RuntimeWarning,\n",
      "/Users/dannychiao/.pyenv/versions/3.7.10/envs/python-3.7/lib/python3.7/site-packages/pyspark/sql/pandas/utils.py:35: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing window\n",
      "              event_timestamp  daily_miles_driven                    created\n",
      "driver_id                                                                   \n",
      "1001      2023-03-06 12:00:00          709.549807 2022-05-14 05:54:04.133501\n",
      "1002      2023-03-06 12:00:00          484.523544 2022-05-14 05:54:04.133501\n",
      "1003      2023-03-06 12:00:00          818.795884 2022-05-14 05:54:04.133501\n",
      "1004      2023-03-06 11:00:00          494.831386 2022-05-14 05:54:04.133501\n",
      "1005      2023-03-06 11:00:00          556.985853 2022-05-14 05:54:04.133501\n",
      "Num rows: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dannychiao/.pyenv/versions/3.7.10/envs/python-3.7/lib/python3.7/site-packages/feast/feature_store.py:1223: RuntimeWarning: Push source is an experimental feature. This API is unstable and it could and might change in the future. We do not guarantee that future changes will maintain backward compatibility.\n",
      "  RuntimeWarning,\n",
      "/Users/dannychiao/.pyenv/versions/3.7.10/envs/python-3.7/lib/python3.7/site-packages/pyspark/sql/pandas/utils.py:35: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing window\n",
      "              event_timestamp  daily_miles_driven                    created\n",
      "driver_id                                                                   \n",
      "1001      2023-03-08 22:00:00          642.781653 2022-05-14 05:54:15.907547\n",
      "1002      2023-03-08 23:00:00          753.619536 2022-05-14 05:54:15.907547\n",
      "1003      2023-03-08 22:00:00          626.791159 2022-05-14 05:54:15.907547\n",
      "1004      2023-03-08 23:00:00          639.276020 2022-05-14 05:54:15.907547\n",
      "1005      2023-03-08 23:00:00          590.422572 2022-05-14 05:54:15.907547\n",
      "Num rows: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dannychiao/.pyenv/versions/3.7.10/envs/python-3.7/lib/python3.7/site-packages/feast/feature_store.py:1223: RuntimeWarning: Push source is an experimental feature. This API is unstable and it could and might change in the future. We do not guarantee that future changes will maintain backward compatibility.\n",
      "  RuntimeWarning,\n"
     ]
    }
   ],
   "source": [
    "def send_to_feast(df, epoch):\n",
    "    pandas_df: pd.DataFrame = df.toPandas()\n",
    "    if pandas_df.empty:\n",
    "        return\n",
    "    \n",
    "    if \"end\" in pandas_df:\n",
    "        print(\"processing window\")\n",
    "        # Filter out only for the latest window for the driver id\n",
    "        pandas_df = pandas_df.sort_values(by=[\"driver_id\",\"end\"], ascending=False).groupby(\"driver_id\").nth(-1)\n",
    "        pandas_df = pandas_df.rename(columns = {\"end\": \"event_timestamp\"})\n",
    "        pandas_df['created'] = pd.to_datetime('now')\n",
    "        store.push(\"driver_stats_push_source\", pandas_df)\n",
    "    pandas_df.sort_values(by=\"driver_id\", inplace=True)\n",
    "    print(pandas_df.head(20))\n",
    "    print(f\"Num rows: {len(pandas_df.index)}\")\n",
    "\n",
    "daily_miles_driven = (\n",
    "    df.withWatermark(\"event_timestamp\", \"1 second\") \n",
    "        .groupBy(\"driver_id\", window(timeColumn=\"event_timestamp\", windowDuration=\"1 day\", slideDuration=\"1 hour\"))\n",
    "        .agg(sum(\"miles_driven\").alias(\"daily_miles_driven\"))\n",
    "        .select(\"driver_id\", \"window.end\", \"daily_miles_driven\")\n",
    ")\n",
    "\n",
    "query_1 = daily_miles_driven \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/feast-workshop/q1/\") \\\n",
    "    .trigger(processingTime=\"15 seconds\") \\\n",
    "    .foreachBatch(send_to_feast) \\\n",
    "    .start()\n",
    "\n",
    "query_1.awaitTermination(timeout=30)\n",
    "query_1.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5b. Verify fresh features\n",
    "Now we can verify that the `daily_miles_driven` feature has indeed changed from the original materialized features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_rate  :  [0.4075707495212555]\n",
      "conv_rate  :  [0.4045884609222412]\n",
      "daily_miles_driven  :  [642.7816772460938]\n",
      "driver_id  :  [1001]\n"
     ]
    }
   ],
   "source": [
    "features = store.get_online_features(\n",
    "    features=[\n",
    "        \"driver_hourly_stats:conv_rate\",\n",
    "        \"driver_hourly_stats:acc_rate\",\n",
    "        \"driver_daily_features:daily_miles_driven\",\n",
    "    ],\n",
    "    entity_rows=[\n",
    "        {\n",
    "            \"driver_id\": 1001,\n",
    "        }\n",
    "    ],\n",
    ").to_dict()\n",
    "print_online_features(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "Finally, let's clean up the checkpoint directory from Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "dir_path = '/tmp/feast-workshop/q1/'\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(dir_path)\n",
    "except OSError as e:\n",
    "    print(\"Error: %s : %s\" % (dir_path, e.strerror))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa281768287d6a9f4d6a9f38bdad903cf8826d866a1bab281f248e07fa174c1b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('python-3.7')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
