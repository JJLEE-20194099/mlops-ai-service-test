{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Serving fresh online features with Feast, Kafka, Redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview\n",
    "In this notebook, we explore using Spark to build streaming features from events in Kafka and registering them within Feast. We then showcase how Feast combines these streaming features with batch data sources in the online store (Redis). Users can then retrieve features at low latency from Redis through Feast.\n",
    "\n",
    "If you haven't already, look at the [README](../README.md) for setup instructions prior to starting this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../architecture.png\" width=\"750\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Spark Structured Streaming to read this Kafka Topic\n",
    "We first read in the events, apply the schema, run some transformations, and `forEachBatch` push to Feast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages=org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/dannychiao/.pyenv/versions/3.8.10/envs/python-3.8-test/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /Users/dannychiao/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/dannychiao/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0cd56c01-abbb-4413-b97b-d05708c82e1c;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/dannychiao/.pyenv/versions/3.8.10/envs/python-3.8-test/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in local-m2-cache\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in local-m2-cache\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 402ms :: artifacts dl 11ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from local-m2-cache in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from local-m2-cache in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0cd56c01-abbb-4413-b97b-d05708c82e1c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/9ms)\n",
      "22/05/15 22:31:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"feast-spark\").getOrCreate()\n",
    "# Reduce partitions since default is 200 which will be slow on a local machine\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "\n",
    "schema = (\n",
    "    StructType()\n",
    "        .add('driver_id', IntegerType(), False)\n",
    "        .add('miles_driven', DoubleType(), False)\n",
    "        .add('event_timestamp', TimestampType(), False)\n",
    "        .add('conv_rate', DoubleType(), False)\n",
    "        .add('acc_rate', DoubleType(), False)\n",
    ")\n",
    "\n",
    "# Subscribe to 1 topic, with headers\n",
    "df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"subscribe\", \"drivers\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    "    .selectExpr('CAST(value AS STRING)')\n",
    "    .select(from_json('value', schema).alias(\"temp\"))\n",
    "    .select(\"temp.*\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Setup the feature store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply feature repository\n",
    "We first run `feast apply` to register the data sources + features and setup Redis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created entity \u001b[1m\u001b[32mdriver\u001b[0m\n",
      "Created feature view \u001b[1m\u001b[32mdriver_daily_features\u001b[0m\n",
      "Created feature view \u001b[1m\u001b[32mdriver_hourly_stats\u001b[0m\n",
      "Created feature service \u001b[1m\u001b[32mmodel_v2\u001b[0m\n",
      "\n",
      "Deploying infrastructure for \u001b[1m\u001b[32mdriver_daily_features\u001b[0m\n",
      "Deploying infrastructure for \u001b[1m\u001b[32mdriver_hourly_stats\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!feast apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we instantiate a Feast `FeatureStore` object to push data to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast import FeatureStore\n",
    "from datetime import datetime\n",
    "\n",
    "store = FeatureStore(repo_path=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch training data from offline store\n",
    "Just to verify the features are in the batch sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      driver_id                  event_timestamp  conv_rate  acc_rate  \\\n",
      "360        1001        2021-04-12 10:59:42+00:00   0.521149  0.751659   \n",
      "721        1002        2021-04-12 08:12:10+00:00   0.089014  0.212637   \n",
      "1084       1003        2021-04-12 16:40:26+00:00   0.188855  0.344736   \n",
      "1445       1004        2021-04-12 15:01:12+00:00   0.296492  0.935305   \n",
      "1805       1001 2022-05-15 22:31:43.743618+00:00   0.404588  0.407571   \n",
      "\n",
      "      daily_miles_driven  \n",
      "360            18.926695  \n",
      "721            12.005569  \n",
      "1084           23.490234  \n",
      "1445           19.204191  \n",
      "1805          350.650257  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "entity_df = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"driver_id\": [1001, 1002, 1003, 1004, 1001],\n",
    "        \"event_timestamp\": [\n",
    "            datetime(2021, 4, 12, 10, 59, 42),\n",
    "            datetime(2021, 4, 12, 8, 12, 10),\n",
    "            datetime(2021, 4, 12, 16, 40, 26),\n",
    "            datetime(2021, 4, 12, 15, 1, 12),\n",
    "            datetime.now()\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"driver_hourly_stats:conv_rate\",\n",
    "        \"driver_hourly_stats:acc_rate\",\n",
    "        \"driver_daily_features:daily_miles_driven\"\n",
    "    ],\n",
    ").to_df()\n",
    "print(training_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Materialize batch features & fetch online features from Redis\n",
    "First we materialize features (which generate the latest values for each entity key from batch sources) into the online store (Redis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materializing \u001b[1m\u001b[32m2\u001b[0m feature views to \u001b[1m\u001b[32m2022-05-14 20:00:00-04:00\u001b[0m into the \u001b[1m\u001b[32mredis\u001b[0m online store.\n",
      "\n",
      "\u001b[1m\u001b[32mdriver_daily_features\u001b[0m from \u001b[1m\u001b[32m1748-07-31 02:31:49-04:56:02\u001b[0m to \u001b[1m\u001b[32m2022-05-14 20:00:00-04:00\u001b[0m:\n",
      "100%|████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 569.40it/s]\n",
      "\u001b[1m\u001b[32mdriver_hourly_stats\u001b[0m from \u001b[1m\u001b[32m1748-07-31 02:31:49-04:56:02\u001b[0m to \u001b[1m\u001b[32m2022-05-14 20:00:00-04:00\u001b[0m:\n",
      "100%|███████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 1248.38it/s]\n"
     ]
    }
   ],
   "source": [
    "!feast materialize-incremental $(date +%Y-%m-%d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can retrieve these features from Redis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_rate  :  [0.4075707495212555]\n",
      "conv_rate  :  [0.4045884609222412]\n",
      "daily_miles_driven  :  [350.6502685546875]\n",
      "driver_id  :  [1001]\n"
     ]
    }
   ],
   "source": [
    "features = store.get_online_features(\n",
    "    features=[\n",
    "        \"driver_hourly_stats:conv_rate\",\n",
    "        \"driver_hourly_stats:acc_rate\",\n",
    "        \"driver_daily_features:daily_miles_driven\",\n",
    "    ],\n",
    "    entity_rows=[\n",
    "        {\n",
    "            \"driver_id\": 1001,\n",
    "        }\n",
    "    ],\n",
    ").to_dict()\n",
    "\n",
    "def print_online_features(features):\n",
    "    for key, value in sorted(features.items()):\n",
    "        print(key, \" : \", value)\n",
    "\n",
    "print_online_features(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generating fresher features via stream transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. Building streaming features with Kafka + Spark Structured Streaming\n",
    "Now we push streaming features into Feast by ingesting events from Kafka and processing with Spark Structured Streaming.\n",
    "- These features can then be further post-processed and combined with other features or request data in on demand transforms.\n",
    "- An example might be to push in the last 5 transactions, and in on demand transforms generate the average of those transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/15 22:31:58 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing window\n",
      "              event_timestamp  daily_miles_driven  \\\n",
      "driver_id                                           \n",
      "1001      2022-04-11 08:00:00           18.926695   \n",
      "1002      2022-04-11 08:00:00           12.005569   \n",
      "1003      2022-04-11 08:00:00           23.490234   \n",
      "1004      2022-04-11 08:00:00           19.204191   \n",
      "1005      2022-04-11 08:00:00            5.764504   \n",
      "\n",
      "                                   created  \n",
      "driver_id                                   \n",
      "1001      2022-05-16 02:32:07.615260+00:00  \n",
      "1002      2022-05-16 02:32:07.615260+00:00  \n",
      "1003      2022-05-16 02:32:07.615260+00:00  \n",
      "1004      2022-05-16 02:32:07.615260+00:00  \n",
      "1005      2022-05-16 02:32:07.615260+00:00  \n",
      "Num rows: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dannychiao/.pyenv/versions/3.8.10/envs/python-3.8-test/lib/python3.8/site-packages/feast/feature_store.py:1219: RuntimeWarning: Push source is an experimental feature. This API is unstable and it could and might change in the future. We do not guarantee that future changes will maintain backward compatibility.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing window\n",
      "              event_timestamp  daily_miles_driven  \\\n",
      "driver_id                                           \n",
      "1001      2023-02-28 23:00:00          643.105436   \n",
      "1002      2023-03-01 00:00:00          692.949960   \n",
      "1003      2023-02-28 23:00:00          458.982921   \n",
      "1004      2023-02-28 23:00:00          576.267295   \n",
      "1005      2023-02-28 23:00:00          610.125993   \n",
      "\n",
      "                                   created  \n",
      "driver_id                                   \n",
      "1001      2022-05-16 02:32:08.775926+00:00  \n",
      "1002      2022-05-16 02:32:08.775926+00:00  \n",
      "1003      2022-05-16 02:32:08.775926+00:00  \n",
      "1004      2022-05-16 02:32:08.775926+00:00  \n",
      "1005      2022-05-16 02:32:08.775926+00:00  \n",
      "Num rows: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dannychiao/.pyenv/versions/3.8.10/envs/python-3.8-test/lib/python3.8/site-packages/feast/feature_store.py:1219: RuntimeWarning: Push source is an experimental feature. This API is unstable and it could and might change in the future. We do not guarantee that future changes will maintain backward compatibility.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing window\n",
      "              event_timestamp  daily_miles_driven  \\\n",
      "driver_id                                           \n",
      "1001      2023-03-01 08:00:00          631.783610   \n",
      "1002      2023-03-01 09:00:00          668.233169   \n",
      "1003      2023-03-01 09:00:00          589.335585   \n",
      "1004      2023-03-01 08:00:00          615.372216   \n",
      "1005      2023-03-01 09:00:00          644.637307   \n",
      "\n",
      "                                   created  \n",
      "driver_id                                   \n",
      "1001      2022-05-16 02:32:15.851802+00:00  \n",
      "1002      2022-05-16 02:32:15.851802+00:00  \n",
      "1003      2022-05-16 02:32:15.851802+00:00  \n",
      "1004      2022-05-16 02:32:15.851802+00:00  \n",
      "1005      2022-05-16 02:32:15.851802+00:00  \n",
      "Num rows: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dannychiao/.pyenv/versions/3.8.10/envs/python-3.8-test/lib/python3.8/site-packages/feast/feature_store.py:1219: RuntimeWarning: Push source is an experimental feature. This API is unstable and it could and might change in the future. We do not guarantee that future changes will maintain backward compatibility.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def send_to_feast(df, epoch):\n",
    "    pandas_df: pd.DataFrame = df.toPandas()\n",
    "    if pandas_df.empty:\n",
    "        return\n",
    "    \n",
    "    if \"end\" in pandas_df:\n",
    "        print(\"processing window\")\n",
    "        # Filter out only for the latest window for the driver id\n",
    "        pandas_df = pandas_df.sort_values(by=[\"driver_id\",\"end\"], ascending=False).groupby(\"driver_id\").nth(-1)\n",
    "        pandas_df = pandas_df.rename(columns = {\"end\": \"event_timestamp\"})\n",
    "        pandas_df['created'] = pd.to_datetime('now', utc=True)\n",
    "        store.push(\"driver_stats_push_source\", pandas_df)\n",
    "    pandas_df.sort_values(by=\"driver_id\", inplace=True)\n",
    "    print(pandas_df.head(20))\n",
    "    print(f\"Num rows: {len(pandas_df.index)}\")\n",
    "\n",
    "daily_miles_driven = (\n",
    "    df.withWatermark(\"event_timestamp\", \"1 second\") \n",
    "        .groupBy(\"driver_id\", window(timeColumn=\"event_timestamp\", windowDuration=\"1 day\", slideDuration=\"1 hour\"))\n",
    "        .agg(sum(\"miles_driven\").alias(\"daily_miles_driven\"))\n",
    "        .select(\"driver_id\", \"window.end\", \"daily_miles_driven\")\n",
    ")\n",
    "\n",
    "query_1 = daily_miles_driven \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/feast-workshop/q1/\") \\\n",
    "    .trigger(processingTime=\"15 seconds\") \\\n",
    "    .foreachBatch(send_to_feast) \\\n",
    "    .start()\n",
    "\n",
    "query_1.awaitTermination(timeout=30)\n",
    "query_1.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5b. Verify fresh features\n",
    "Now we can verify that the `daily_miles_driven` feature has indeed changed from the original materialized features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_rate  :  [0.4075707495212555]\n",
      "conv_rate  :  [0.4045884609222412]\n",
      "daily_miles_driven  :  [631.7836303710938]\n",
      "driver_id  :  [1001]\n"
     ]
    }
   ],
   "source": [
    "features = store.get_online_features(\n",
    "    features=[\n",
    "        \"driver_hourly_stats:conv_rate\",\n",
    "        \"driver_hourly_stats:acc_rate\",\n",
    "        \"driver_daily_features:daily_miles_driven\",\n",
    "    ],\n",
    "    entity_rows=[\n",
    "        {\n",
    "            \"driver_id\": 1001,\n",
    "        }\n",
    "    ],\n",
    ").to_dict()\n",
    "print_online_features(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "Finally, let's clean up the checkpoint directory from Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "dir_path = '/tmp/feast-workshop/q1/'\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(dir_path)\n",
    "except OSError as e:\n",
    "    print(\"Error: %s : %s\" % (dir_path, e.strerror))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "897235db2a882d4808b8b7306d9885bcbc83a97fa026b35b0c5378d9bfffccbb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('python-3.8-test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
